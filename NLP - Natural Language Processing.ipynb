{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\asus\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (4.47.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "#natural language processing Tool Kit (Doğal dil araç kiti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#sent_tokenize ne işe yarar? (cümle)\n",
    "#paragraftan cümleleri ayırıp liste haline getirir.\n",
    "#bir kitabı da versen cümleleri ayırıp liste yapar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "#ihtiyacımız olan paketi indirdik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome readers.', 'I hope you find it interesting.', 'Please do reply.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Welcome readers. I hope you find it interesting. Please do reply.\"\n",
    "print(sent_tokenize(text))\n",
    "#tokenize parçalara ayır demek sonra da listeye çevirir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PierreVinken', ',', '59', 'years', 'old', ',', 'will', 'join', 'as', 'a', 'nonexecutive', 'director', 'or', 'Nov.29', '.']\n"
     ]
    }
   ],
   "source": [
    "#word_tokenize cümleden kelimeleri ayırarak liste haline getirir.\n",
    "text = nltk.word_tokenize(\"PierreVinken, 59 years old , will join as a nonexecutive director or Nov.29.\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do',\n",
       " \"n't\",\n",
       " 'have',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'day.',\n",
       " 'I',\n",
       " ',',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'find',\n",
       " 'the',\n",
       " 'book',\n",
       " 'interesting']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(\"Don't have a nice day. I, hope you find the book interesting\")\n",
    "#word_tokenize ile aynı işlemi yapar.\n",
    "#. ve boşluk koyunca anlıyor ve yeni cümle olarak anlaması için büyük harfle başlaman lazım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "#stopwords : gereksiz kelimeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MPT = nltk.WordPunctTokenizer()\n",
    "stop_word_list = nltk.corpus.stopwords.words('turkish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acaba',\n",
       " 'ama',\n",
       " 'aslında',\n",
       " 'az',\n",
       " 'bazı',\n",
       " 'belki',\n",
       " 'biri',\n",
       " 'birkaç',\n",
       " 'birşey',\n",
       " 'biz',\n",
       " 'bu',\n",
       " 'çok',\n",
       " 'çünkü',\n",
       " 'da',\n",
       " 'daha',\n",
       " 'de',\n",
       " 'defa',\n",
       " 'diye',\n",
       " 'eğer',\n",
       " 'en',\n",
       " 'gibi',\n",
       " 'hem',\n",
       " 'hep',\n",
       " 'hepsi',\n",
       " 'her',\n",
       " 'hiç',\n",
       " 'için',\n",
       " 'ile',\n",
       " 'ise',\n",
       " 'kez',\n",
       " 'ki',\n",
       " 'kim',\n",
       " 'mı',\n",
       " 'mu',\n",
       " 'mü',\n",
       " 'nasıl',\n",
       " 'ne',\n",
       " 'neden',\n",
       " 'nerde',\n",
       " 'nerede',\n",
       " 'nereye',\n",
       " 'niçin',\n",
       " 'niye',\n",
       " 'o',\n",
       " 'sanki',\n",
       " 'şey',\n",
       " 'siz',\n",
       " 'şu',\n",
       " 'tüm',\n",
       " 've',\n",
       " 'veya',\n",
       " 'ya',\n",
       " 'yani']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_word_list #Türkçe'deki gereksiz kelimeler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmer and Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stemmer ekleri atar, Lemma kökleri bulur__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hapi'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmerporter = PorterStemmer()\n",
    "stemmerporter.stem('hapiness')\n",
    "#hapiness' in kökünü bulacağız."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hapy'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmerlan = LancasterStemmer()\n",
    "stemmerlan.stem('hapiness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n",
      "hapiness\n",
      "pair\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "stemmerregexp = RegexpStemmer('ing')\n",
    "#Kaldırmak istediğimiz eki parantez içine yazdık.\n",
    "print(stemmerregexp.stem('working'))\n",
    "print(stemmerregexp.stem('hapiness'))\n",
    "print(stemmerregexp.stem('pairing'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n",
      "comiend\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "print(SnowballStemmer.languages)\n",
    "#Burada hangi dilleri desteklediğini görüyoruz.\n",
    "#Başka dillerdeki kelimelerin köklerini bulmamızı sağlar.\n",
    "\n",
    "spanishtemmer = SnowballStemmer(\"spanish\")\n",
    "print(spanishtemmer.stem(\"comiende\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kökleri bulur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n",
      "work\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer_output = WordNetLemmatizer()\n",
    "print(lemmatizer_output.lemmatize('working'))\n",
    "#working'i aynen bastırdı çünkü bu ing ekiyle farklı bir anlamı olan bir kelime\n",
    "#pos = part of speech(Cümlenin ögeleri)\n",
    "#pos ile bu kelimenin fiil(verb) olduğunu söylüyoruz.\n",
    "print(lemmatizer_output.lemmatize('working',pos='v'))\n",
    "#working' in fiil olduğunu biliyoruz kökünü bul diyoruz.\n",
    "print(lemmatizer_output.lemmatize('works'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences between Stemming and Lemmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happi\n",
      "men\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer_output = PorterStemmer()\n",
    "print(stemmer_output.stem('happiness'))\n",
    "print(stemmer_output.stem('Mens'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mens'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer_output=WordNetLemmatizer()\n",
    "lemmatizer_output.lemmatize('Mens')\n",
    "#stem ile ekleri keser\n",
    "#went' i go' ya çeviriyorsa kök buluyordur ve bunu stemma yapar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging and POS Tagger\n",
    "Özne mi nesne mi vs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Drive',\n",
       " 'into',\n",
       " 'NLTK',\n",
       " ':',\n",
       " 'Part-of-speech',\n",
       " 'tagging',\n",
       " 'and',\n",
       " 'POS',\n",
       " 'Tagger']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = word_tokenize('Drive into NLTK: Part-of-speech tagging and POS Tagger')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Drive', 'NNP'),\n",
       " ('into', 'IN'),\n",
       " ('NLTK', 'NNP'),\n",
       " (':', ':'),\n",
       " ('Part-of-speech', 'JJ'),\n",
       " ('tagging', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('POS', 'NNP'),\n",
       " ('Tagger', 'NNP')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NNP ,CC,IN...__ anlamlarını öğrenmek için Google'da nltk.pos_tag list olarak aratırsak karşımıza çıkacaktır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('pleasant', 'JJ'),\n",
       " ('day', 'NN'),\n",
       " ('today', 'NN')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = word_tokenize('It is a pleasant day today')\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Beautiful', 'He is the man'), ('morning', 'He is the man')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger\n",
    "tag = DefaultTagger('He is the man')\n",
    "#hangisinin ne olduğunu söyler(ne olarak etiketlemek istediğimizi yazıyoruz)\n",
    "#beautiful ve morningi he is the man olarak etiketledik.\n",
    "tag.tag(['Beautiful','morning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language conversion & Text formatting & Grammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect\n",
    "#yanlış yazılmış kelimeleri düzelteceğiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import spell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell('Tghe') # Burada 'the' kelimesini düzeltti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob\n",
    "Bir kelimenin hangi dilden olduğunu anlayabiliyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from textblob) (3.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (0.16.0)\n",
      "Requirement already satisfied: regex in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2020.6.8)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.47.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --trusted-host pypi.python.org textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "#hangi dilde konuştuğumuzu anlar correct ile de düzeltir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "I have good spelling!\n"
     ]
    }
   ],
   "source": [
    "b = TextBlob('I havv good spelling!')\n",
    "print(b.detect_language())\n",
    "print(b.correct())\n",
    "#en = english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fallibility', 0.3333333333333333),\n",
       " ('capability', 0.3333333333333333),\n",
       " ('affability', 0.3333333333333333)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=Word('falability')\n",
    "w.spellcheck()\n",
    "#Burada yazımı yanlış olabilen kelimelerin doğru olabilecek ihtimallerini verir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.0.8)\n",
      "Requirement already satisfied: six in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langdetect) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect\n",
    "#Yazılan cümle veya kelimelerin hangi dilden olduğunu verir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "de\n",
      "es\n",
      "tr\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "print(detect(\"War doesn't show who's right, just who's left\"))\n",
    "print(detect(\"Ein, zwei, drei, vier\"))\n",
    "print(detect(\"Eu gosto de vier\"))\n",
    "print(detect(\"Nasılsınız\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Eu sou um homem negro livre amado por Jesus Cristo.\")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_blob = TextBlob(u'I am a free black man loved by Jesus Christ.')\n",
    "en_blob.translate(to='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"I have a lot of work.\")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_blob = TextBlob(u'Benim çok işim var.')\n",
    "tr_blob.translate(to='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Term Frequency - Inverse Document Frequency\n",
    "TF = Kelimenin kaç defa geçtiği manasına gelir.\n",
    "\n",
    "IDF = Herhangi bir text döküman vs içerisinde belirli kelime kaç defa geçtiğini yüzde olarak hesaplar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Kelimelerin bir dökümanda ne kadar tekrar ettiğine bakar. Google web sitelerinin anasayfasından bir kelimenin ne kadar tekrar ettiğini buluyor\n",
    "\n",
    "#### - Aradığın kelime %20 den fazla bağlantılıysa o siteyle ilişki kurup sana yönlendiriyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_train=['Call you tonight','Call me a cab','please call me... PLEASE!']\n",
    "vect = CountVectorizer()\n",
    "#cümle içinde geçen her kelime için bir sütun açtı\n",
    "#Cümle de o kelime geçerse 1 yazar\n",
    "#fit ile öğrenir. transform ise 1-0 yazmaya yarar\n",
    "#toarray bunu diziye çevirir.\n",
    "#columns'ta sütun isimlerini alıp başına koyar\n",
    "#varsa 1 eklendi yoksa 0 koydu\n",
    "tf = pd.DataFrame(vect.fit_transform(simple_train).toarray(),columns=vect.get_feature_names())\n",
    "#fit_transform ile sütun isimleri kaybolur\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    1     3   2       1        1    1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect=CountVectorizer(binary=True)\n",
    "df = vect.fit_transform(simple_train).toarray().sum(axis=0)\n",
    "pd.DataFrame(df.reshape(1,6),columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab      call   me  please  tonight  you\n",
       "0  0.0  0.333333  0.0     0.0      1.0  1.0\n",
       "1  1.0  0.333333  0.5     0.0      0.0  0.0\n",
       "2  0.0  0.333333  0.5     2.0      0.0  0.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf/df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652491</td>\n",
       "      <td>0.652491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266075</td>\n",
       "      <td>0.342620</td>\n",
       "      <td>0.901008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cab      call        me    please   tonight       you\n",
       "0  0.000000  0.385372  0.000000  0.000000  0.652491  0.652491\n",
       "1  0.720333  0.425441  0.547832  0.000000  0.000000  0.000000\n",
       "2  0.000000  0.266075  0.342620  0.901008  0.000000  0.000000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect=TfidfVectorizer()\n",
    "pd.DataFrame(vect.fit_transform(simple_train).toarray(),columns=vect.get_feature_names())\n",
    "#BÜtün kelimeleri çok boyutlu bir uzayda vektör olarak düşünüyoruz ve bu kelimeleri 0 noktasına olan uzaklığı hesaplıyor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer-Fit Transform with NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>documents</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  document  documents  first  is  one  second  the  third  this\n",
       "0    0         1          0      1   1    0       0    1      0     1\n",
       "1    0         0          1      0   1    0       2    0      0     1\n",
       "2    1         0          0      0   0    1       0    1      1     0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer=CountVectorizer(min_df=1)\n",
    "\n",
    "print(vectorizer)\n",
    "corpus=['This is the first document','This is second second documents','And the third one']\n",
    "x=vectorizer.fit_transform(corpus)\n",
    "tf=pd.DataFrame(x.toarray(),columns=vectorizer.get_feature_names())\n",
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp review analysis\n",
    "Yelp diye bir restaurant hakkında yazılmış yorumların analizini yapmaya başlayacağız. Yazılan yorumlar negatif mi pozitif mi bunu anlamaya çalışacağız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp = pd.read_csv('yelp.csv')\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   business_id  10000 non-null  object\n",
      " 1   date         10000 non-null  object\n",
      " 2   review_id    10000 non-null  object\n",
      " 3   stars        10000 non-null  int64 \n",
      " 4   text         10000 non-null  object\n",
      " 5   type         10000 non-null  object\n",
      " 6   user_id      10000 non-null  object\n",
      " 7   cool         10000 non-null  int64 \n",
      " 8   useful       10000 non-null  int64 \n",
      " 9   funny        10000 non-null  int64 \n",
      "dtypes: int64(4), object(6)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "yelp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restaurant adına yazılan yorumun pozitif mi negatif mi olduğunu anlayacağız."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP PROJE ADIMLARI\n",
    "\n",
    "__1-__Bütün cümleler küçük harfe çevirilir.\n",
    "\n",
    "__2-__Noktalama işaretleri kaldırılır.\n",
    "\n",
    "__3-__Rakamları kaldır.\n",
    "\n",
    "__4-__Satır sonu,\\n enter a basılmışsa \\r\n",
    "\n",
    "__5-__stop words(gereksiz kelimeler)'leri kaldırç\n",
    "\n",
    "__6-__Tokenize işlemi yapıyoruz.\n",
    "\n",
    "__7-__Lemma ve stemma uygula(ekleri kaldırıp kökleri bul)\n",
    "\n",
    "__8-__vectorizer ile yazıları rakama atıyoruz(dummie_variables gibi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp['text'] = yelp['text'].str.lower()\n",
    "#CÜmleleri küçük harfe çevirir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp['text'] = yelp['text'].str.replace('[^\\w\\s]','')\n",
    "#Noktalama işaretlerini kaldırır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp['text'] = yelp['text'].str.replace('[\\d+]','')\n",
    "#Rakamları kaldırır(rakamları boşluk ile değiştirir.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp['text'] = yelp['text'].str.replace('\\n','').replace('\\r','')\n",
    "#satır sonu işaretleri kaldırılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>my wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>i have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate rice is so good and i also...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>rosie dakota and i love chaparral dog park its...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>general manager scott petello is a good egg no...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  my wife took me here on my birthday for breakf...  review   \n",
       "1  i have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate rice is so good and i also...  review   \n",
       "3  rosie dakota and i love chaparral dog park its...  review   \n",
       "4  general manager scott petello is a good egg no...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4086,)\n"
     ]
    }
   ],
   "source": [
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "#Burada 5 yıldız verenler ile 1 yıldız verenleri ayırdık(yani veri olarak onları aldık.) Çünkü 5 yıldız verenlerin pozitif 1 yıldız verenlerin negatif konuştuğunu biliyoruz.\n",
    "\n",
    "yelp_best_worst.reset_index(drop=True,inplace=True)\n",
    "#indexleri sıfırlayıp yeniden indexliyoruz\n",
    "x=yelp_best_worst.text\n",
    "y=yelp_best_worst.stars\n",
    "print(x.shape)\n",
    "\n",
    "x_train, x_test, y_train, y_test= train_test_split(x,y,random_state=42)\n",
    "#10000? satırlık veri 4086 oldu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       my wife took me here on my birthday for breakf...\n",
      "1       i have no idea why some people give bad review...\n",
      "2       rosie dakota and i love chaparral dog park its...\n",
      "3       general manager scott petello is a good egg no...\n",
      "4       drop what youre doing and drive here after i a...\n",
      "                              ...                        \n",
      "4081    yes i do rock the hipster joints  i dig this p...\n",
      "4082    only  stars a few notes the folks that rated t...\n",
      "4083    im not normally one to jump at reviewing a cha...\n",
      "4084    lets seewhat is there not to like about surpri...\n",
      "4085     locations all  star average i think arizona r...\n",
      "Name: text, Length: 4086, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       5\n",
      "1       5\n",
      "2       5\n",
      "3       5\n",
      "4       5\n",
      "       ..\n",
      "4081    5\n",
      "4082    5\n",
      "4083    5\n",
      "4084    5\n",
      "4085    5\n",
      "Name: stars, Length: 4086, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Şimdi biz bu projede eğer yeni birisi yorum yazarsa acaba kaç yıldız verir onu bulacağız."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2113)\t1\n",
      "  (0, 17234)\t1\n",
      "  (0, 7774)\t1\n",
      "  (0, 1798)\t1\n",
      "  (0, 5661)\t1\n",
      "  (0, 20153)\t1\n",
      "  (0, 10576)\t1\n",
      "  (0, 17261)\t1\n",
      "  (0, 16168)\t1\n",
      "  (0, 9039)\t1\n",
      "  (0, 10636)\t1\n",
      "  (0, 19350)\t1\n",
      "  (0, 10674)\t1\n",
      "  (0, 13521)\t1\n",
      "  (0, 509)\t1\n",
      "  (0, 17935)\t1\n",
      "  (0, 16097)\t1\n",
      "  (0, 14583)\t1\n",
      "  (0, 12027)\t1\n",
      "  (0, 20116)\t1\n",
      "  (0, 15939)\t1\n",
      "  (0, 10661)\t1\n",
      "  (0, 6156)\t1\n",
      "  (0, 15946)\t1\n",
      "  (1, 20153)\t1\n",
      "  :\t:\n",
      "  (3062, 19917)\t1\n",
      "  (3062, 5578)\t1\n",
      "  (3062, 18740)\t1\n",
      "  (3062, 12233)\t1\n",
      "  (3063, 15000)\t1\n",
      "  (3063, 7855)\t2\n",
      "  (3063, 13546)\t1\n",
      "  (3063, 4767)\t1\n",
      "  (3063, 9580)\t1\n",
      "  (3063, 12000)\t1\n",
      "  (3063, 7192)\t1\n",
      "  (3063, 9014)\t1\n",
      "  (3063, 16988)\t1\n",
      "  (3063, 10767)\t1\n",
      "  (3063, 3057)\t1\n",
      "  (3063, 1676)\t1\n",
      "  (3063, 1989)\t1\n",
      "  (3063, 8409)\t1\n",
      "  (3063, 16322)\t1\n",
      "  (3063, 19278)\t1\n",
      "  (3063, 3083)\t1\n",
      "  (3063, 2135)\t1\n",
      "  (3063, 17070)\t1\n",
      "  (3063, 15104)\t1\n",
      "  (3063, 11004)\t1\n"
     ]
    }
   ],
   "source": [
    "vect=CountVectorizer(lowercase=True,stop_words='english',)\n",
    "x_train_dtm=vect.fit_transform(x_train)\n",
    "print(x_train_dtm)\n",
    "x_test_dtm=vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599     from my door its a five minute stroll through ...\n",
      "752     this is by far the best chicken fried steak ar...\n",
      "3148    ok so us  hot chix got all dolled up for dinne...\n",
      "1001    located in the space formerly occupied by lgos...\n",
      "2515    does anyone know why their regular subs are  m...\n",
      "                              ...                        \n",
      "1941    alasive given up the idea of writing a review ...\n",
      "1350    this has got to be the best barber shop ive ev...\n",
      "927     simple but really great cajun food prepared in...\n",
      "612     awesome food anyone go check out modern steak ...\n",
      "1498    i was very disappointed in my encounter with t...\n",
      "Name: text, Length: 1022, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>______i</th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaamazingthey</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aathis</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abbazabba</th>\n",
       "      <th>...</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zucchiniwe</th>\n",
       "      <th>zuccini</th>\n",
       "      <th>zuchinni</th>\n",
       "      <th>zumba</th>\n",
       "      <th>zupa</th>\n",
       "      <th>zupas</th>\n",
       "      <th>zuzu</th>\n",
       "      <th>école</th>\n",
       "      <th>ém</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ______i  aa  aaa  aaaamazingthey  aaron  aathis  ab  abandoned  abandoning  \\\n",
       "0        0   0    0               0      0       0   0          0           0   \n",
       "1        0   0    0               0      0       0   0          0           0   \n",
       "2        0   0    0               0      0       0   0          0           0   \n",
       "3        0   0    0               0      0       0   0          0           0   \n",
       "4        0   0    0               0      0       0   0          0           0   \n",
       "\n",
       "   abbazabba  ...  zucchini  zucchiniwe  zuccini  zuchinni  zumba  zupa  \\\n",
       "0          0  ...         0           0        0         0      0     0   \n",
       "1          0  ...         0           0        0         0      0     0   \n",
       "2          0  ...         0           0        0         0      0     0   \n",
       "3          0  ...         0           0        0         0      0     0   \n",
       "4          0  ...         0           0        0         0      0     0   \n",
       "\n",
       "   zupas  zuzu  école  ém  \n",
       "0      0     0      0   0  \n",
       "1      0     0      0   0  \n",
       "2      0     0      0   0  \n",
       "3      0     0      0   0  \n",
       "4      0     0      0   0  \n",
       "\n",
       "[5 rows x 20512 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf=pd.DataFrame(x_train_dtm.toarray(),columns=vect.get_feature_names())\n",
    "tf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2456    my boyfriend and i stopped by to grab a bite t...\n",
       "2562    when was the last time that you went to a rest...\n",
       "219     handsdown my favorite pizza place in the east ...\n",
       "3199    arai pastry is so deliciousironically i come h...\n",
       "485     great mompop store weve stopped by several tim...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 20809)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect=CountVectorizer()\n",
    "x_train_dtm=vect.fit_transform(x_train)\n",
    "x_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 174168)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect=CountVectorizer(ngram_range=(1,2))\n",
    "#ngram kelimenin öncesi veya sonrası. yani 2 kelimeye bakacak.\n",
    "x_train_dtm=vect.fit_transform(x_train)\n",
    "x_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zoners', 'zoners out', 'zones', 'zones dolls', 'zones so', 'zoning', 'zoning issues', 'zoo', 'zoo but', 'zoo if', 'zoo is', 'zoo not', 'zoo the', 'zoo tour', 'zoom', 'zoom in', 'zoyo', 'zoyo for', 'zucca', 'zucca appetizer', 'zucchini', 'zucchini and', 'zucchini broccoli', 'zucchini carrots', 'zucchini fires', 'zucchini fries', 'zucchini pieces', 'zucchini strips', 'zucchini very', 'zucchini with', 'zucchiniwe', 'zucchiniwe all', 'zuccini', 'zuccini italian', 'zuchinni', 'zuchinni again', 'zumba', 'zumba class', 'zumba or', 'zumba yogalates', 'zupa', 'zupa flavors', 'zupas', 'zupas cater', 'zuzu', 'zuzu was', 'école', 'école lenôtre', 'ém', 'ém all']\n"
     ]
    }
   ],
   "source": [
    "print(vect.get_feature_names()[-50:])#son 50 kelime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the star rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect=CountVectorizer()\n",
    "x_train_dtm=vect.fit_transform(x_train)\n",
    "x_test_dtm=vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8972602739726028\n"
     ]
    }
   ],
   "source": [
    "nb=MultinomialNB()\n",
    "nb.fit(x_train_dtm,y_train)\n",
    "y_pred_class=nb.predict(x_test_dtm)\n",
    "\n",
    "print(metrics.accuracy_score(y_test,y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate null accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_test(vect):\n",
    "    x_train_dtm = vect.fit_transform(x_train)\n",
    "    print('Features: ',x_train_dtm.shape[1])\n",
    "    x_test_dtm=vect.transform(x_test)\n",
    "    nb=MultinomialNB()\n",
    "    nb.fit(x_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(x_test_dtm)\n",
    "    print('Accuracy: ',metrics.accuracy_score(y_test, y_pred_class))\n",
    "#burada ngram_range bir kelimeye iki kelime yaptık.(Öncesi sonrası) Bunu yaptığımızda ve yapmadığımızdaki başarı oranlarını karşılaştırıyoruz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  174168\n",
      "Accuracy:  0.8317025440313112\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  20809\n",
      "Accuracy:  0.8972602739726028\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
